{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4eVyIJesz6XA",
    "outputId": "defeaeb0-1487-4393-cfc1-b204d0157c90"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import joblib\n",
    "# Download the WordNet resource\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdAZRhHP0huo"
   },
   "outputs": [],
   "source": [
    "training=pd.read_csv(\"twitter_training.csv\")\n",
    "validation=pd.read_csv(\"twitter_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9hUb5Nl030u"
   },
   "outputs": [],
   "source": [
    "training.columns=[\"id\",\"branch\",\"sentiment\",\"tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3QV02gW04M7"
   },
   "outputs": [],
   "source": [
    "training=training.drop('id',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjc2wrNr06mo"
   },
   "outputs": [],
   "source": [
    "training = training.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CfzeHa009rp"
   },
   "outputs": [],
   "source": [
    "validation.columns=[\"id\",\"branch\",\"sentiment\",\"tweet\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7xqgFSz1-LY"
   },
   "outputs": [],
   "source": [
    "validation=validation.drop('id',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqOi0-Ue6m4e"
   },
   "outputs": [],
   "source": [
    "# Cleaning function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J0Jb0Kgm2S1O",
    "outputId": "e2e0fe11-e0ba-48df-d901-1b48fcf856ff"
   },
   "outputs": [],
   "source": [
    "# Tokenization, stop words removal, stemming, and lemmatization\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply cleaning and preprocessing\n",
    "training['cleaned_text'] = training['tweet'].apply(clean_text)\n",
    "training['processed_text'] = training['cleaned_text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eRHp3-Ee6AsU"
   },
   "outputs": [],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZeJAKtd39FH"
   },
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(training['processed_text'], training['sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "jSCKQeU641S4",
    "outputId": "8cc36585-8c5d-4175-ff0f-bf22ffb8bc53"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Tokenize for Word2Vec\n",
    "tokenized_sentences = [sentence.split() for sentence in X_train]\n",
    "\n",
    "# Word2Vec model (CBOW)\n",
    "cbow_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4, sg=0)\n",
    "\n",
    "# Word2Vec model (Skip-gram)\n",
    "skipgram_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
    "\n",
    "# Get average word vectors for sentences, ensuring handling of empty slices\n",
    "def get_avg_word2vec(sentence, model):\n",
    "    words = sentence.split()\n",
    "    word_vecs = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(word_vecs) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(word_vecs, axis=0)\n",
    "\n",
    "# Applying the function and ensuring homogeneous arrays\n",
    "X_train_cbow = np.array([get_avg_word2vec(sentence, cbow_model) for sentence in X_train])\n",
    "X_test_cbow = np.array([get_avg_word2vec(sentence, cbow_model) for sentence in X_test])\n",
    "\n",
    "X_train_skipgram = np.array([get_avg_word2vec(sentence, skipgram_model) for sentence in X_train])\n",
    "X_test_skipgram = np.array([get_avg_word2vec(sentence, skipgram_model) for sentence in X_test])\n",
    "\n",
    "print(f\"X_train_cbow shape: {X_train_cbow.shape}\")\n",
    "print(f\"X_test_cbow shape: {X_test_cbow.shape}\")\n",
    "print(f\"X_train_skipgram shape: {X_train_skipgram.shape}\")\n",
    "print(f\"X_test_skipgram shape: {X_test_skipgram.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ifUBmJ1I47xs"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "vv5UyNFz5qH6"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "cNE3ZLKvIojm"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "def train_and_evaluate(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    return acc, f1, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "jHj5ZZxNIzUg"
   },
   "outputs": [],
   "source": [
    "# Modèles séparés\n",
    "model_SVM = SVC()\n",
    "model_NaiveBayes = MultinomialNB()\n",
    "model_LogisticRegression = LogisticRegression(max_iter=200)\n",
    "model_AdaBoost = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Evaluating Naive Bayes with CBOW Word2Vec\")\n",
    "results_NaiveBayes_cbow = train_and_evaluate(model_NaiveBayes, X_train_cbow, X_test_cbow, y_train, y_test)\n",
    "\n",
    "print(\"Evaluating Naive Bayes with Skip-gram Word2Vec\")\n",
    "results_NaiveBayes_skipgram = train_and_evaluate(model_NaiveBayes, X_train_skipgram, X_test_skipgram, y_train, y_test)\n",
    "\n",
    "print(\"Evaluating Naive Bayes with BOW\")\n",
    "results_NaiveBayes_bow = train_and_evaluate(model_NaiveBayes, X_train_bow, X_test_bow, y_train, y_test)\n",
    "\n",
    "print(\"Evaluating Naive Bayes with TF-IDF\")\n",
    "results_NaiveBayes_tfidf = train_and_evaluate(model_NaiveBayes, X_train_tfidf, X_test_tfidf, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "print(\"Evaluating Logistic Regression with CBOW Word2Vec\")\n",
    "results_LogisticRegression_cbow = train_and_evaluate(model_LogisticRegression, X_train_cbow, X_test_cbow, y_train, y_test)\n",
    "\n",
    "print(\"Evaluating Logistic Regression with Skip-gram Word2Vec\")\n",
    "results_LogisticRegression_skipgram = train_and_evaluate(model_LogisticRegression, X_train_skipgram, X_test_skipgram, y_train, y_test)\n",
    "\n",
    "print(\"Evaluating Logistic Regression with BOW\")\n",
    "results_LogisticRegression_bow = train_and_evaluate(model_LogisticRegression, X_train_bow, X_test_bow, y_train, y_test)\n",
    "\n",
    "print(\"Evaluating Logistic Regression with TF-IDF\")\n",
    "results_LogisticRegression_tfidf = train_and_evaluate(model_LogisticRegression, X_train_tfidf, X_test_tfidf, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "print(\"Evaluating AdaBoost with CBOW Word2Vec\")\n",
    "results_AdaBoost_cbow = train_and_evaluate(model_AdaBoost, X_train_cbow, X_test_cbow, y_train, y_test)\n",
    "\n",
    "print(\"Evaluating AdaBoost with Skip-gram Word2Vec\")\n",
    "results_AdaBoost_skipgram = train_and_evaluate(model_AdaBoost, X_train_skipgram, X_test_skipgram, y_train, y_test)\n",
    "\n",
    "print(\"Evaluating AdaBoost with BOW\")\n",
    "results_AdaBoost_bow = train_and_evaluate(model_AdaBoost, X_train_bow, X_test_bow, y_train, y_test)\n",
    "\n",
    "print(\"Evaluating AdaBoost with TF-IDF\")\n",
    "results_AdaBoost_tfidf = train_and_evaluate(model_AdaBoost, X_train_tfidf, X_test_tfidf, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    # 'SVM_cbow': results_SVM_cbow,\n",
    "    # 'SVM_skipgram': results_SVM_skipgram,\n",
    "    # 'SVM_bow': results_SVM_bow,\n",
    "    # 'SVM_tfidf': results_SVM_tfidf,\n",
    "    # 'NaiveBayes_cbow': results_NaiveBayes_cbow,\n",
    "    # 'NaiveBayes_skipgram': results_NaiveBayes_skipgram,\n",
    "    # 'NaiveBayes_bow': results_NaiveBayes_bow,\n",
    "    # 'NaiveBayes_tfidf': results_NaiveBayes_tfidf,\n",
    "    'LogisticRegression_cbow': results_LogisticRegression_cbow,\n",
    "    'LogisticRegression_skipgram': results_LogisticRegression_skipgram,\n",
    "    'LogisticRegression_bow': results_LogisticRegression_bow,\n",
    "    'LogisticRegression_tfidf': results_LogisticRegression_tfidf,\n",
    "    'AdaBoost_cbow': results_AdaBoost_cbow,\n",
    "    'AdaBoost_skipgram': results_AdaBoost_skipgram,\n",
    "    'AdaBoost_bow': results_AdaBoost_bow,\n",
    "    'AdaBoost_tfidf': results_AdaBoost_tfidf\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in results.items():\n",
    "    print(f\"Model: {key}\\nAccuracy: {value[0]}\\nF1 Score: {value[1]}\\nReport:\\n{value[2]}\\n\")\n",
    "\n",
    "# Example of interpreting results:\n",
    "best_model = max(results, key=lambda k: results[k][1])  # Choose the model with the highest F1 score\n",
    "print(f\"The best model is {best_model} with F1 Score of {results[best_model][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGjvYgjGCAUT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
